{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "fa52835e",
      "metadata": {
        "id": "fa52835e"
      },
      "source": [
        "Before I answer the questions, it is important for me to note that due to lack of memory in the computer (locally), I had to train the model on a remote server. That's why I'm attaching two files. The first file is a Jupyter Notebook file, in order to run it you need to change the path in cell number 3. And the second file is a regular .py file that I wrote to train on the remote server. Thanks and sorry for the inconvenience.\n",
        "I will attach all the necessary files to GitHub."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9328cf82",
      "metadata": {
        "id": "9328cf82"
      },
      "source": [
        "Question 1\n",
        "\n",
        "Image segmentation is a method in which a digital image is broken down into various subgroups called Image segments which helps in reducing the complexity of the image to make further processing or analysis of the image simpler. Segmentation in easy words is assigning labels to pixels.\n",
        "\n",
        "Similarity approach: This approach is based on detecting similarity between image pixels to form a segment, based on a threshold. ML algorithms like clustering are based on this type of approach to segment an image.\n",
        "Discontinuity approach: This approach relies on the discontinuity of pixel intensity values of the image. Line, Point, and Edge Detection techniques use this type of approach for obtaining intermediate segmentation results which can be later processed to obtain the final segmented image.\n",
        "\n",
        "\n",
        "Types of Image Segmentation tasks:\n",
        "\n",
        "\n",
        "1) Image segmentation tasks can be classified into three groups based on the amount and type of information they convey.\n",
        "\n",
        "\n",
        "2) Semantic segmentation refers to the classification of pixels in an image into semantic classes. Pixels belonging to a particular class are simply \n",
        "\n",
        "classified to that class with no other information or context taken into consideration. \n",
        "\n",
        "\n",
        "3) Instance segmentation models classify pixels into categories on the basis of “instances” rather than classes. \n",
        "\n",
        "\n",
        "4) Panoptic segmentation, the most recently developed segmentation task, can be expressed as the combination of semantic segmentation and instance segmentation where each instance of an object in the image is segregated and the object’s identity is predicted. \n",
        "\n",
        "\n",
        "According to studies and articles, the best approach to solve a segmentation problem is through the use of the UNET network.\n",
        "What exactly is a UNET network?\n",
        "\n",
        "\n",
        "UNET is a U-shaped encoder-decoder network architecture, which consists of four encoder blocks and four decoder blocks that are connected via a bridge. The encoder network (contracting path) half the spatial dimensions and double the number of filters (feature channels) at each encoder block. Likewise, the decoder network doubles the spatial dimensions and half the number of feature channels.\n",
        "\n",
        "\n",
        "The encoder network acts as the feature extractor and learns an abstract representation of the input image through a sequence of the encoder blocks.\n",
        "These skip connections provide additional information that helps the decoder to generate better semantic features. They also act as a shortcut connection that helps the indirect flow of gradients to the earlier layers without any degradation.\n",
        "\n",
        "\n",
        "The bridge connects the encoder and the decoder network and completes the flow of information.\n",
        "\n",
        "\n",
        "The decoder network is used to take the abstract representation and generate a semantic segmentation mask.\n",
        "\n",
        "\n",
        "In our case, I decided to use MobileNetV2 as the Encoder (the left part of the U shape). MobileNetV2 is an architecture that is optimized for mobile devices. It improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes.\n",
        "\n",
        "\n",
        "Advantages of using MobileNetV2 as an Encoder:\n",
        "\n",
        "\n",
        "1) MobileNetV2 has less parameters, due to which it is easy to train.\n",
        "\n",
        "\n",
        "2) Using a pre-trained encoder helps the model to converge much faster in comparison to the non-pretrained model.\n",
        "\n",
        "\n",
        "3) A pre-trained encoder helps the model to achieve high performance as compared to a non pre-trained model.\n",
        "\n",
        "\n",
        "As a loss function, I used dice loss. Dice Loss is widely used in medical image segmentation tasks to address the data imbalance problem. However, it only addresses the imbalance problem between foreground and background yet overlooks another imbalance between easy and hard examples that also severely affects the training process of a learning model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51ea7fb5",
      "metadata": {
        "id": "51ea7fb5"
      },
      "source": [
        "Question 2\n",
        "\n",
        "\n",
        "The base model I used is SegNet, generally composed of Convolutional Encoder-Decoder. I then built the UNET in order to use a more complicated network that aims to give better results.\n",
        "\n",
        "\n",
        "The Dice coefficient is very similar to the IoU. They are positively correlated, meaning if one says model A is better than model B at segmenting an image, then the other will say the same. Like the IoU, they both range from 0 to 1, with 1 signifying the greatest similarity between predicted and truth. The most commonly used metrics for semantic segmentation are the IoU and the Dice Coefficient.\n",
        "\n",
        "\n",
        "At first I trained the UNET network from Scretch. Then in order to improve results I decided to use transfer learning in order to use an already trained network in order to get better results. The results with the help of transfer learning are indeed better, after the convergence of the model I tried to improve beyond that through fine tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efdf0999",
      "metadata": {
        "id": "efdf0999"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import glob\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tensorflow.keras.layers import Conv2D, Activation, BatchNormalization\n",
        "from tensorflow.keras.layers import UpSampling2D, Input, Concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.metrics import Recall, Precision\n",
        "from tensorflow.keras import backend as K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b092507",
      "metadata": {
        "id": "9b092507"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "#Initializing parameters\n",
        "IMAGE_SIZE = 256\n",
        "EPOCHS = 15\n",
        "BATCH = 16\n",
        "LR = 1e-4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19d8f1a8",
      "metadata": {
        "id": "19d8f1a8"
      },
      "outputs": [],
      "source": [
        "BASE_OUTPUT = '/mnt/md0/mory/DUTS-TR/'\n",
        "PLOT_PATH = os.path.sep.join([BASE_OUTPUT,\n",
        "\t\"loss_plot.png\"])\n",
        "\n",
        "images_path ='/mnt/md0/mory/DUTS-TR/DUTS-TR-Image'\n",
        "mask_path = '/mnt/md0/mory/DUTS-TR/DUTS-TR-Mask'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca9740d4",
      "metadata": {
        "id": "ca9740d4"
      },
      "outputs": [],
      "source": [
        "#Sorting the names of the files\n",
        "images_names = sorted(\n",
        "    [\n",
        "        os.path.join(images_path, fname)\n",
        "        for fname in os.listdir(images_path)\n",
        "        if fname.endswith(\".jpg\")\n",
        "    ]\n",
        ")\n",
        "mask_names = sorted(\n",
        "    [\n",
        "        os.path.join(mask_path, fname)\n",
        "        for fname in os.listdir(mask_path)\n",
        "        if fname.endswith(\".png\") and not fname.startswith(\".\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(len(images_names))\n",
        "print(len(mask_names))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9334949f",
      "metadata": {
        "id": "9334949f"
      },
      "outputs": [],
      "source": [
        "#Load data\n",
        "print(\"Loading Images...\")\n",
        "\n",
        "#Capture training image info as a list\n",
        "train_images = []\n",
        "\n",
        "for directory_path in glob.glob(images_path):\n",
        "    for img_path in glob.glob(os.path.join(directory_path, \"*.jpg\")):\n",
        "        img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
        "        img = cv2.resize(img, (IMAGE_SIZE, IMAGE_SIZE))\n",
        "        img = img/255.0   \n",
        "        train_images.append(img)\n",
        "        \n",
        "train_images = np.array(train_images)        \n",
        "print(train_images.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3a43344",
      "metadata": {
        "id": "c3a43344"
      },
      "outputs": [],
      "source": [
        "print(\"Loading Masks...\")\n",
        "\n",
        "#Capture mask/label info as a list\n",
        "train_masks = [] \n",
        "\n",
        "for directory_path in glob.glob(mask_path):\n",
        "    for mask_path in glob.glob(os.path.join(directory_path, \"*.png\")):\n",
        "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "        mask = cv2.resize(mask, (IMAGE_SIZE, IMAGE_SIZE))\n",
        "        mask = np.expand_dims(mask, axis = -1)\n",
        "        mask = mask/255.0\n",
        "        #mask = cv2.resize(mask, (SIZE_Y, SIZE_X), interpolation = cv2.INTER_NEAREST)  #Otherwise ground truth changes due to interpolation\n",
        "        train_masks.append(mask)\n",
        "        \n",
        "#Convert list to array for machine learning processing          \n",
        "train_masks = np.array(train_masks)\n",
        "print(train_masks.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f22df01",
      "metadata": {
        "id": "8f22df01"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "#Picking 20% for testing and remaining for training\n",
        "train_x, test_x, train_y, test_y = train_test_split(train_images, train_masks, test_size = 0.20, random_state = 42)\n",
        "\n",
        "#Checking the shapes after the split\n",
        "print(train_x.shape)\n",
        "print(test_x.shape)\n",
        "print(train_y.shape)\n",
        "print(test_y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad38605e",
      "metadata": {
        "id": "ad38605e"
      },
      "outputs": [],
      "source": [
        "#Defining model architecture \n",
        "def model():\n",
        "    inputs = Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3), name=\"input_image\")\n",
        "    \n",
        "    encoder = MobileNetV2(input_tensor=inputs, weights=\"imagenet\", include_top=False, alpha=0.35)\n",
        "    skip_connection_names = [\"input_image\", \"block_1_expand_relu\", \"block_3_expand_relu\", \"block_6_expand_relu\"]\n",
        "    encoder_output = encoder.get_layer(\"block_13_expand_relu\").output\n",
        "    \n",
        "    f = [16, 32, 48, 64]\n",
        "    x = encoder_output\n",
        "    for i in range(1, len(skip_connection_names)+1, 1):\n",
        "        x_skip = encoder.get_layer(skip_connection_names[-i]).output\n",
        "        x = UpSampling2D((2, 2))(x)\n",
        "        x = Concatenate()([x, x_skip])\n",
        "        \n",
        "        x = Conv2D(f[-i], (3, 3), padding=\"same\")(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Activation(\"relu\")(x)\n",
        "        \n",
        "        x = Conv2D(f[-i], (3, 3), padding=\"same\")(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Activation(\"relu\")(x)\n",
        "        \n",
        "    x = Conv2D(1, (1, 1), padding=\"same\")(x)\n",
        "    x = Activation(\"sigmoid\")(x)\n",
        "    \n",
        "    model = Model(inputs, x)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "476a553f",
      "metadata": {
        "id": "476a553f"
      },
      "outputs": [],
      "source": [
        "#Defining loss function \n",
        "smooth = 1e-15\n",
        "def dice_coef(y_true, y_pred):\n",
        "    y_true = tf.keras.layers.Flatten()(y_true)\n",
        "    y_pred = tf.keras.layers.Flatten()(y_pred)\n",
        "    intersection = tf.reduce_sum(y_true * y_pred)\n",
        "    return (2. * intersection + smooth) / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) + smooth)\n",
        "\n",
        "def dice_loss(y_true, y_pred):\n",
        "    return 1.0 - dice_coef(y_true, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b93947a6",
      "metadata": {
        "id": "b93947a6"
      },
      "outputs": [],
      "source": [
        "model = model()\n",
        "model.summary()  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bdd25f92",
      "metadata": {
        "id": "bdd25f92"
      },
      "outputs": [],
      "source": [
        "#Compiling the model\n",
        "print(\"[INFO] compiling model...\")\n",
        "opt = tf.keras.optimizers.Adam(LR)\n",
        "metrics = [dice_coef, Recall(), Precision()]\n",
        "model.compile(loss=dice_loss, optimizer=opt, metrics=metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3d2211d",
      "metadata": {
        "id": "f3d2211d"
      },
      "outputs": [],
      "source": [
        "input_shape = (128, 128, 3)\n",
        "model = U_Net(input_shape)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56503fef",
      "metadata": {
        "id": "56503fef"
      },
      "outputs": [],
      "source": [
        "#Defining callbacks\n",
        "callbacks = [\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=4),\n",
        "    EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=False),\n",
        "    WandbCallback()\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "771ce7bb",
      "metadata": {
        "id": "771ce7bb"
      },
      "outputs": [],
      "source": [
        "print(\"[INFO] training model...\")\n",
        "history=model.fit(\n",
        "    train_x, \n",
        "    train_y,\n",
        "    batch_size=BATCH, \n",
        "    epochs=EPOCHS,\n",
        "    verbose=1,\n",
        "    validation_data=(test_x, test_y),\n",
        "    callbacks=callbacks)\n",
        "\n",
        "print(\"[INFO] saving model...\")\n",
        "model.save(\"model_seg.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e986b551",
      "metadata": {
        "id": "e986b551"
      },
      "outputs": [],
      "source": [
        "def plot_training(H, plotPath):\n",
        "\t# construct a plot that plots and saves the training history\n",
        "    plt.style.use(\"ggplot\")\n",
        "    plt.figure()\n",
        "    plt.plot(H.history[\"loss\"], label=\"train_loss\")\n",
        "    plt.plot(H.history[\"val_loss\"], label=\"val_loss\")\n",
        "    plt.plot(H.history[\"dice_coef\"], label=\"train_acc\")\n",
        "    plt.plot(H.history[\"val_dice_coef\"], label=\"val_acc\")\n",
        "    plt.title(\"Training Loss and Accuracy\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Loss/Accuracy\")\n",
        "    plt.legend(loc=\"lower left\")\n",
        "    plt.savefig(plotPath)\n",
        "\n",
        "plot_training(history, PLOT_PATH)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9cebffe8",
      "metadata": {
        "id": "9cebffe8"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "colab": {
      "name": "Avi_AI Assignment_AngelList.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}